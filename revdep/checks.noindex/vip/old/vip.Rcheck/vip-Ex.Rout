
R version 4.0.0 (2020-04-24) -- "Arbor Day"
Copyright (C) 2020 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin17.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "vip"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('vip')

Attaching package: â€˜vipâ€™

The following object is masked from â€˜package:utilsâ€™:

    vi

> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("gen_friedman")
> ### * gen_friedman
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gen_friedman
> ### Title: Friedman benchmark data
> ### Aliases: gen_friedman
> 
> ### ** Examples
> 
> gen_friedman()
            y         x1         x2         x3         x4          x5
1   16.314030 0.26550866 0.65472393 0.26750821 0.67371223 0.658877609
2    7.440210 0.37212390 0.35319727 0.21864528 0.09485786 0.185069965
3   14.259454 0.57285336 0.27026015 0.51679684 0.49259612 0.954378137
4   13.216988 0.90820779 0.99268406 0.26895059 0.46155184 0.897848492
5   14.509800 0.20168193 0.63349326 0.18116833 0.37521653 0.943697054
6   19.356630 0.89838968 0.21320814 0.51857614 0.99109922 0.723690751
7    7.302707 0.94467527 0.12937235 0.56278294 0.17635071 0.370357066
8   23.139210 0.66079779 0.47811803 0.12915685 0.81343521 0.781017540
9   11.718503 0.62911404 0.92407447 0.25636760 0.06844664 0.011149509
10  10.704158 0.06178627 0.59876097 0.71793528 0.40044975 0.940308712
11  16.290244 0.20597457 0.97617069 0.96140994 0.14114433 0.993749226
12  10.772769 0.17655675 0.73179251 0.10014085 0.19330986 0.357405745
13  20.401614 0.68702285 0.35672691 0.76322269 0.84135172 0.747635063
14  20.156423 0.38410372 0.43147369 0.94796635 0.71991399 0.792909024
15  11.699871 0.76984142 0.14821156 0.81863469 0.26721208 0.705859006
16   8.291790 0.49769924 0.01307758 0.30829233 0.49500164 0.475825039
17  13.700658 0.71761851 0.71556607 0.64957946 0.08311390 0.494654526
18  12.387411 0.99190609 0.10318424 0.95335545 0.35388424 0.308052449
19  22.327710 0.38003518 0.44628435 0.95373265 0.96920881 0.695012246
20  20.991978 0.77744522 0.64010105 0.33997920 0.62471419 0.822793306
21  12.145373 0.93470523 0.99183862 0.26247411 0.66461825 0.434717641
22  11.209278 0.21214252 0.49559358 0.16545393 0.31248966 0.514732653
23  16.285009 0.65167377 0.48434952 0.32216806 0.40568961 0.663010968
24  11.383055 0.12555510 0.17344233 0.51012521 0.99607737 0.143166587
25  19.784544 0.26722067 0.75482094 0.92396847 0.85508236 0.344487394
26  16.630079 0.38611409 0.45389549 0.51095970 0.95354840 0.405763582
27   9.925149 0.01339033 0.51116978 0.25762126 0.81230509 0.085311006
28  19.184160 0.38238796 0.20754511 0.04646089 0.78218212 0.932571928
29  12.922682 0.86969085 0.22865814 0.41785626 0.26787813 0.838384067
30  20.486874 0.34034900 0.59571200 0.85400150 0.76215153 0.879433296
31  22.536654 0.48208012 0.57487220 0.34723068 0.98631159 0.935712468
32   7.578463 0.59956583 0.07706438 0.13144232 0.29360555 0.072460633
33   6.761079 0.49354131 0.03554058 0.37448686 0.39935111 0.378759441
34  14.783364 0.18621760 0.64279549 0.63142023 0.81213152 0.537864923
35   8.355440 0.82737332 0.92861520 0.39007893 0.07715167 0.105050139
36  17.796291 0.66846674 0.59809242 0.68962785 0.36369681 0.801687706
37  18.725745 0.79423986 0.56090075 0.68941341 0.44259247 0.739641746
38   3.790735 0.10794363 0.52602772 0.55490062 0.15671413 0.052149013
39  16.237089 0.72371095 0.98509522 0.42962441 0.58220527 0.482169573
40  20.416960 0.41127443 0.50764182 0.45272006 0.97016218 0.920517841
41  20.629808 0.82094629 0.68278808 0.30644326 0.98949983 0.041528429
42  12.793402 0.64706019 0.60154122 0.57835394 0.17645204 0.293991799
43  16.888089 0.78293276 0.23886868 0.91037030 0.54213042 0.500850487
44  13.784797 0.55303631 0.25816593 0.14260408 0.38430389 0.609748935
45  17.728192 0.52971958 0.72930962 0.41504763 0.67616405 0.264249050
46  15.484133 0.78935623 0.45257083 0.21092575 0.26929378 0.423098610
47   6.685187 0.02333120 0.17512677 0.42875037 0.46925094 0.366563616
48  18.181404 0.47723007 0.74669827 0.13268998 0.17180008 0.942505322
49   6.514141 0.73231374 0.10498764 0.46009645 0.36918946 0.123723565
50  21.088843 0.69273156 0.86454495 0.94295706 0.72540527 0.070032679
51  19.075104 0.47761962 0.61464497 0.76197386 0.48614910 0.964317038
52  16.556239 0.86120948 0.55715954 0.93290983 0.06380247 0.442510108
53  13.974491 0.43809711 0.32877732 0.47067850 0.78454623 0.370272380
54   8.624261 0.24479728 0.45313145 0.60358807 0.41832164 0.170243580
55  11.349540 0.07067905 0.50044097 0.48498968 0.98101808 0.054190429
56   9.669153 0.09946616 0.18086636 0.10880632 0.28288396 0.657828069
57  17.432145 0.31627171 0.52963060 0.24772683 0.84788215 0.578161917
58   7.062593 0.51863426 0.07527575 0.49851453 0.08223923 0.987101764
59  17.617352 0.66200508 0.27775593 0.37286671 0.88645875 0.603792401
60  11.457241 0.40683019 0.21269952 0.93469137 0.47193073 0.064949919
61   9.080746 0.91287592 0.28479048 0.52398608 0.10910096 0.162109082
62  13.725911 0.29360337 0.89509410 0.31714467 0.33327798 0.475397920
63  15.440402 0.45906573 0.44623532 0.27796603 0.83741657 0.001932835
64  13.844450 0.33239467 0.77998489 0.78754051 0.27684984 0.441459143
65  17.671116 0.65087047 0.88061903 0.70246251 0.58703514 0.260929737
66  18.699665 0.25801678 0.41312421 0.16502764 0.83673227 0.938413745
67   9.017723 0.47854525 0.06380848 0.06445754 0.07115402 0.715833284
68  16.351474 0.76631067 0.33548749 0.75470562 0.70277874 0.163085478
69  11.500369 0.08424691 0.72372595 0.62041003 0.69882454 0.476188018
70  18.371222 0.87532133 0.33761533 0.16957677 0.46396238 0.690256723
71  16.629386 0.33907294 0.63041412 0.06221405 0.43693111 0.460895180
72  21.524238 0.83944035 0.84061455 0.10902927 0.56217679 0.955146738
73  21.238721 0.34668349 0.85613166 0.38171635 0.92848323 0.712540122
74  10.420267 0.33377493 0.39135928 0.16931091 0.23046641 0.397147933
75   9.093748 0.47635125 0.38049389 0.29865254 0.22181375 0.117720612
76  13.301513 0.89219834 0.89544543 0.19220954 0.42021589 0.240116273
77  18.730864 0.86433947 0.64431576 0.25717002 0.33352081 0.863630567
78  20.945211 0.38998954 0.74107865 0.18123182 0.86480755 0.435976402
79  14.032473 0.77732070 0.60530345 0.47731371 0.17719454 0.497868052
80  13.785268 0.96061800 0.90308161 0.77073704 0.49331873 0.691927672
81  16.325018 0.43465948 0.29373016 0.02778712 0.42971337 0.760313282
82  10.602965 0.71251468 0.19126011 0.52731078 0.56426384 0.155401223
83  22.834623 0.39999437 0.88645094 0.88031907 0.65616232 0.849457093
84  19.930458 0.32535215 0.50333949 0.37306337 0.97855406 0.946817819
85  18.101054 0.75708715 0.87705754 0.04795913 0.23216115 0.588419190
86   8.820857 0.20269226 0.18919362 0.13862825 0.24081160 0.502250815
87  19.464713 0.71112122 0.75810305 0.32149212 0.79683608 0.189779918
88  13.490149 0.12169192 0.72449889 0.15483161 0.83167172 0.001836858
89  14.918180 0.24548851 0.94372482 0.13222817 0.11350771 0.877578062
90  14.221604 0.14330438 0.54764659 0.22130593 0.96331202 0.134111338
91   8.160363 0.23962942 0.71174387 0.22638080 0.14732290 0.022741224
92   9.554971 0.05893438 0.38890510 0.13141653 0.14362694 0.939136706
93  17.515896 0.64228826 0.10087313 0.98156346 0.92522994 0.292948723
94  11.940746 0.87626921 0.92730209 0.32701373 0.50703560 0.164326574
95   9.722642 0.77891468 0.28323250 0.50693950 0.15485102 0.399102556
96  16.474305 0.79730883 0.59057316 0.68144251 0.34830205 0.459575412
97  13.471983 0.45527445 0.11036060 0.09916910 0.65982103 0.434030849
98  17.396794 0.41008408 0.84050703 0.11890256 0.31177237 0.517009826
99  19.122315 0.81087024 0.31796368 0.05043966 0.35157341 0.846245753
100 15.328704 0.60493329 0.78285134 0.92925392 0.14784571 0.055164286
             x6         x7          x8          x9         x10
1   0.554177061 0.81425175 0.929743206 0.858687453 0.831898988
2   0.688275238 0.92877723 0.900939270 0.034438761 0.766842754
3   0.658057554 0.14748105 0.750882188 0.970997146 0.272780316
4   0.663342725 0.74982166 0.676568772 0.745110136 0.188163299
5   0.472234203 0.97565735 0.648013446 0.273255242 0.225761835
6   0.969528166 0.97479246 0.073246870 0.677106102 0.061970368
7   0.402197063 0.35062557 0.423558418 0.347947468 0.059900241
8   0.849552104 0.39394906 0.530824364 0.947020522 0.148155870
9   0.756644907 0.95095101 0.942704762 0.338623775 0.072987886
10  0.532601219 0.10664832 0.712224559 0.031713204 0.042885892
11  0.874149661 0.93476012 0.724490575 0.353585798 0.522461272
12  0.467115116 0.34616210 0.470128618 0.387123588 0.789231489
13  0.008128455 0.53306061 0.120282256 0.356947160 0.694753348
14  0.727767005 0.53879430 0.783097671 0.959968606 0.066576047
15  0.716589479 0.71471795 0.438157397 0.383738205 0.017957747
16  0.187426371 0.40579050 0.431455980 0.546337621 0.442158423
17  0.646067277 0.15278814 0.027497879 0.925323476 0.157602083
18  0.541979280 0.34023276 0.146561843 0.916859527 0.718860116
19  0.335320760 0.62665485 0.422595158 0.242732993 0.703941833
20  0.637908723 0.05737268 0.767137174 0.717090839 0.885226305
21  0.829201064 0.85166764 0.004766445 0.348946036 0.325261015
22  0.708975198 0.21264535 0.603595692 0.554283351 0.970881567
23  0.348550351 0.53946203 0.905577261 0.742972413 0.984820266
24  0.128327875 0.13648759 0.706661532 0.819698319 0.039184718
25  0.388078489 0.32486514 0.262537159 0.869657265 0.893571288
26  0.928177548 0.62107629 0.851076219 0.035865134 0.822399151
27  0.804390771 0.25598225 0.333605515 0.220024403 0.724244693
28  0.758696807 0.63487580 0.578284268 0.366973210 0.288920138
29  0.957249889 0.48567211 0.432773130 0.305696442 0.501948605
30  0.993913879 0.93817692 0.051595323 0.727830458 0.428347001
31  0.606440995 0.85750154 0.729803288 0.699728766 0.610016016
32  0.029377165 0.37088354 0.548170414 0.910060927 0.926113174
33  0.336445358 0.31420183 0.751221986 0.845602610 0.240123258
34  0.277658087 0.82853436 0.050771165 0.778544640 0.271881834
35  0.117197550 0.45184151 0.714966212 0.400843428 0.735003646
36  0.043218259 0.31587841 0.297694457 0.576804659 0.751545873
37  0.370309786 0.09780854 0.283477210 0.076359446 0.930591921
38  0.336878309 0.06490054 0.829877127 0.872974851 0.462730275
39  0.173652553 0.68945737 0.086395375 0.956285736 0.860691177
40  0.621773280 0.66805060 0.042657119 0.505531080 0.312036420
41  0.397843627 0.90454665 0.348740809 0.924366364 0.208039013
42  0.955675769 0.30169327 0.542336010 0.134513533 0.920827467
43  0.653349443 0.93280870 0.609460864 0.238737003 0.052736884
44  0.328743717 0.20198302 0.271372098 0.489962622 0.411155431
45  0.197146714 0.79237876 0.205230932 0.474060097 0.456341215
46  0.115342325 0.22463051 0.381632148 0.434994159 0.415860248
47  0.995965479 0.03075657 0.472558115 0.039248960 0.670353452
48  0.379276725 0.86203404 0.835517950 0.644331015 0.756904080
49  0.561988093 0.68510751 0.121471496 0.715567200 0.911651314
50  0.732718018 0.94207491 0.675808125 0.351666721 0.819952086
51  0.870805553 0.67585376 0.496685220 0.946556749 0.093649853
52  0.572170260 0.84312015 0.902237748 0.743106507 0.177906081
53  0.011036071 0.36189418 0.551263967 0.050785920 0.642214580
54  0.906315261 0.39236589 0.128564677 0.980364781 0.874282994
55  0.770653632 0.56768740 0.442009786 0.217822453 0.592313350
56  0.382504623 0.09515213 0.192297696 0.036196002 0.262432192
57  0.094045889 0.19378407 0.434900622 0.260703865 0.883752264
58  0.049653584 0.58806639 0.225133403 0.720415786 0.187689346
59  0.821162318 0.75150417 0.961096282 0.247894181 0.488866982
60  0.829324304 0.86723879 0.448427465 0.234915792 0.009820487
61  0.654732876 0.37179574 0.777144985 0.021011843 0.360434043
62  0.132827813 0.79881455 0.158219774 0.264620628 0.442161691
63  0.341809901 0.05831439 0.866808583 0.691801619 0.125729221
64  0.731371579 0.62343571 0.206145606 0.034669391 0.624364548
65  0.907291416 0.35664141 0.177949683 0.062786130 0.302431332
66  0.696196999 0.58792793 0.164891146 0.397706092 0.239637228
67  0.241579222 0.91378464 0.565268977 0.490028425 0.358629764
68  0.644107229 0.19944218 0.727181021 0.631048748 0.892910413
69  0.280750214 0.36908362 0.875918989 0.723404312 0.990627265
70  0.957636521 0.67140833 0.708424384 0.076765792 0.732421197
71  0.158395957 0.76814455 0.477463211 0.421446081 0.992788088
72  0.418338992 0.52224828 0.820279994 0.973726686 0.539789362
73  0.252009797 0.82807502 0.016612211 0.813805783 0.433801762
74  0.094390268 0.52709619 0.996111101 0.223767723 0.964673674
75  0.827717770 0.50175483 0.634894836 0.489742306 0.148319643
76  0.525305504 0.41997332 0.428475381 0.011340193 0.066493056
77  0.667747627 0.36229828 0.028392933 0.258602297 0.024251488
78  0.408277760 0.12342891 0.753332701 0.454598772 0.612423242
79  0.842589903 0.29816153 0.208633975 0.740976688 0.696608748
80  0.737305468 0.27667649 0.999455405 0.990660012 0.801141887
81  0.348224401 0.77022536 0.907249193 0.332204946 0.137730284
82  0.948938177 0.77818130 0.712465032 0.944791405 0.159784823
83  0.646679190 0.14378728 0.730932884 0.961853420 0.988833593
84  0.035277767 0.51552615 0.472487921 0.899277500 0.398645696
85  0.596448456 0.59724049 0.862510687 0.492715835 0.187911816
86  0.415318002 0.50584302 0.165415122 0.784935478 0.477728241
87  0.076897038 0.38609963 0.620427926 0.803119047 0.544234023
88  0.528048879 0.42609810 0.292331395 0.677767624 0.328157981
89  0.962333309 0.01175973 0.456141734 0.581277579 0.999930594
90  0.708740051 0.91933177 0.045639209 0.330668087 0.527781836
91  0.553475704 0.07943969 0.177661752 0.001314657 0.133734508
92  0.242956609 0.50737425 0.057932068 0.065801964 0.438540068
93  0.778042529 0.82017162 0.944410234 0.085951917 0.620793940
94  0.651940943 0.59839542 0.342743474 0.013109717 0.202912412
95  0.830245704 0.42415353 0.517950571 0.335745624 0.134169773
96  0.648550947 0.55931027 0.625245335 0.119264317 0.776833998
97  0.479835794 0.78909447 0.237264854 0.593595445 0.635949849
98  0.495064106 0.16771526 0.516123914 0.037312676 0.282136297
99  0.379872591 0.97045173 0.806541828 0.009571792 0.191259456
100 0.450485437 0.47350310 0.348264734 0.161596126 0.265536720
> 
> 
> 
> cleanEx()
> nameEx("list_metrics")
> ### * list_metrics
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: list_metrics
> ### Title: List metrics
> ### Aliases: list_metrics
> 
> ### ** Examples
> 
> (metrics <- list_metrics())
     Metric                       Description                             Task
1  accuracy           Classification accuracy Binary/multiclass classification
2     error           Misclassification error Binary/multiclass classification
3       auc            Area under (ROC) curve            Binary classification
4   logloss                          Log loss            Binary classification
5      mauc Multiclass area under (ROC) curve        Multiclass classification
6       mae               Mean absolute error                       Regression
7       mse                Mean squared error                       Regression
8        r2                         R squared                       Regression
9  rsquared                         R squared                       Regression
10     rmse           Root mean squared error                       Regression
11      sse             Sum of squared errors                       Regression
> metrics[metrics$Task == "Multiclass classification", ]
  Metric                       Description                      Task
5   mauc Multiclass area under (ROC) curve Multiclass classification
> 
> 
> 
> cleanEx()
> nameEx("metrics")
> ### * metrics
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: metric_mse
> ### Title: Model metrics
> ### Aliases: metric_mse metric_rmse metric_sse metric_mae metric_rsquared
> ###   metric_accuracy metric_error metric_auc metric_logLoss metric_mauc
> 
> ### ** Examples
> 
> x <- rnorm(10)
> y <- rnorm(10)
> metric_mse(x, y)
[1] 2.157546
> metric_rsquared(x, y)
[1] 0.1419054
> 
> 
> 
> cleanEx()
> nameEx("vi")
> ### * vi
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vi
> ### Title: Variable importance
> ### Aliases: vi vi.default vi.model_fit vi.WrappedModel vi.Learner
> 
> ### ** Examples
> 
> #
> # A projection pursuit regression example
> #
> 
> # Load the sample data
> data(mtcars)
> 
> # Fit a projection pursuit regression model
> mtcars.ppr <- ppr(mpg ~ ., data = mtcars, nterms = 1)
> 
> # Compute variable importance scores
> vi(mtcars.ppr, method = "firm", ice = TRUE)
[90m# A tibble: 10 x 2[39m
   Variable Importance
   [3m[90m<chr>[39m[23m         [3m[90m<dbl>[39m[23m
[90m 1[39m wt           3.44  
[90m 2[39m hp           2.57  
[90m 3[39m gear         1.85  
[90m 4[39m qsec         1.56  
[90m 5[39m cyl          0.743 
[90m 6[39m am           0.690 
[90m 7[39m vs           0.448 
[90m 8[39m drat         0.245 
[90m 9[39m carb         0.087[4m0[24m
[90m10[39m disp         0.024[4m8[24m
> vi(mtcars.ppr, method = "firm", ice = TRUE,
+    var_fun = list("con" = mad, "cat" = function(x) diff(range(x)) / 4))
[90m# A tibble: 10 x 2[39m
   Variable Importance
   [3m[90m<chr>[39m[23m         [3m[90m<dbl>[39m[23m
[90m 1[39m wt           3.87  
[90m 2[39m hp           2.85  
[90m 3[39m gear         2.14  
[90m 4[39m qsec         1.71  
[90m 5[39m cyl          0.949 
[90m 6[39m am           0.723 
[90m 7[39m vs           0.470 
[90m 8[39m drat         0.297 
[90m 9[39m carb         0.102 
[90m10[39m disp         0.031[4m7[24m
> 
> # Plot variable importance scores
> vip(mtcars.ppr, method = "firm", ice = TRUE)
> 
> 
> 
> cleanEx()
> nameEx("vi_permute")
> ### * vi_permute
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vi_permute
> ### Title: Permutation-based variable importance
> ### Aliases: vi_permute vi_permute.default
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Load required packages
> ##D library(ggplot2)  # for ggtitle() function
> ##D library(nnet)     # for fitting neural networks
> ##D 
> ##D # Simulate training data
> ##D trn <- gen_friedman(500, seed = 101)  # ?vip::gen_friedman
> ##D 
> ##D # Inspect data
> ##D tibble::as.tibble(trn)
> ##D 
> ##D # Fit PPR and NN models (hyperparameters were chosen using the caret package
> ##D # with 5 repeats of 5-fold cross-validation)
> ##D pp <- ppr(y ~ ., data = trn, nterms = 11)
> ##D set.seed(0803) # for reproducibility
> ##D nn <- nnet(y ~ ., data = trn, size = 7, decay = 0.1, linout = TRUE,
> ##D            maxit = 500)
> ##D 
> ##D # Plot VI scores
> ##D set.seed(2021)  # for reproducibility
> ##D p1 <- vip(pp, method = "permute", target = "y", metric = "rsquared",
> ##D           pred_wrapper = predict) + ggtitle("PPR")
> ##D p2 <- vip(nn, method = "permute", target = "y", metric = "rsquared",
> ##D           pred_wrapper = predict) + ggtitle("NN")
> ##D grid.arrange(p1, p2, ncol = 2)
> ##D 
> ##D # Mean absolute error
> ##D mae <- function(actual, predicted) {
> ##D   mean(abs(actual - predicted))
> ##D }
> ##D 
> ##D # Permutation-based VIP with user-defined MAE metric
> ##D set.seed(1101)  # for reproducibility
> ##D vip(pp, method = "permute", target = "y", metric = mae,
> ##D     smaller_is_better = TRUE,
> ##D     pred_wrapper = function(object, newdata) predict(object, newdata)
> ##D ) + ggtitle("PPR")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("vint")
> ### * vint
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vint
> ### Title: Interaction effects
> ### Aliases: vint
> 
> ### ** Examples
> 
> ## Not run: 
> ##D #
> ##D # The Friedman 1 benchmark problem
> ##D #
> ##D 
> ##D # Load required packages
> ##D library(gbm)
> ##D library(ggplot2)
> ##D library(mlbench)
> ##D 
> ##D # Simulate training data
> ##D trn <- gen_friedman(500, seed = 101)  # ?vip::gen_friedman
> ##D 
> ##D #
> ##D # NOTE: The only interaction that actually occurs in the model from which
> ##D # these data are generated is between x.1 and x.2!
> ##D #
> ##D 
> ##D # Fit a GBM to the training data
> ##D set.seed(102)  # for reproducibility
> ##D fit <- gbm(y ~ ., data = trn, distribution = "gaussian", n.trees = 1000,
> ##D            interaction.depth = 2, shrinkage = 0.01, bag.fraction = 0.8,
> ##D            cv.folds = 5)
> ##D best_iter <- gbm.perf(fit, plot.it = FALSE, method = "cv")
> ##D 
> ##D # Quantify relative interaction strength
> ##D all_pairs <- combn(paste0("x.", 1:10), m = 2)
> ##D res <- NULL
> ##D for (i in seq_along(all_pairs)) {
> ##D   interact <- vint(fit, feature_names = all_pairs[, i], n.trees = best_iter)
> ##D   res <- rbind(res, interact)
> ##D }
> ##D 
> ##D # Plot top 20 results
> ##D top_20 <- res[1L:20L, ]
> ##D ggplot(top_20, aes(x = reorder(Variables, Interaction), y = Interaction)) +
> ##D   geom_col() +
> ##D   coord_flip() +
> ##D   xlab("") +
> ##D   ylab("Interaction strength")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("vip")
> ### * vip
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vip
> ### Title: Variable importance plots
> ### Aliases: vip vip.default vip.model_fit
> 
> ### ** Examples
> 
> #
> # A projection pursuit regression example
> #
> 
> # Load the sample data
> data(mtcars)
> 
> # Fit a projection pursuit regression model
> model <- ppr(mpg ~ ., data = mtcars, nterms = 1)
> 
> # Construct variable importance plot
> vip(model, method = "firm")
> 
> # Better yet, store the variable importance scores and then plot
> vi_scores <- vi(model, method = "firm")
> vip(vi_scores, geom = "point", horiz = FALSE)
> vip(vi_scores, geom = "point", horiz = FALSE, aesthetics = list(size = 3))
> 
> # The `%T>%` operator is imported for convenience; see ?magrittr::`%T>%`
> # for details
> vi_scores <- model %>%
+   vi(method = "firm") %T>%
+   {print(vip(.))}
> vi_scores
[90m# A tibble: 10 x 2[39m
   Variable Importance
   [3m[90m<chr>[39m[23m         [3m[90m<dbl>[39m[23m
[90m 1[39m wt           3.41  
[90m 2[39m hp           2.54  
[90m 3[39m gear         1.82  
[90m 4[39m qsec         1.52  
[90m 5[39m cyl          0.725 
[90m 6[39m am           0.663 
[90m 7[39m vs           0.433 
[90m 8[39m drat         0.236 
[90m 9[39m carb         0.084[4m4[24m
[90m10[39m disp         0.024[4m2[24m
> 
> # Permutation scores (barplot w/ raw values and jittering)
> pfun <- function(object, newdata) predict(object, newdata = newdata)
> vip(model, method = "permute", train = mtcars, target = "mpg", nsim = 10,
+     metric = "rmse", pred_wrapper = pfun,
+     aesthetics = list(color = "grey50", fill = "grey50"),
+     all_permutations = TRUE, jitter = TRUE)
> 
> # Permutation scores (boxplot)
> vip(model, method = "permute", train = mtcars, target = "mpg", nsim = 10,
+     metric = "rmse", pred_wrapper = pfun, geom = "boxplot")
> 
> # Permutation scores (boxplot colored by feature)
> library(ggplot2)  # for `aes_string()` function
> vip(model, method = "permute", train = mtcars, target = "mpg", nsim = 10,
+     metric = "rmse", pred_wrapper = pfun, geom = "boxplot",
+     all_permutations = TRUE, mapping = aes_string(fill = "Variable"),
+     aesthetics = list(color = "grey35", size = 0.8))
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()

detaching â€˜package:ggplot2â€™

> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  3.137 0.17 3.508 0.002 0.005 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
